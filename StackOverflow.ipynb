{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # 1. Purpose of the project","metadata":{}},{"cell_type":"markdown","source":"This project will focus on data collected from stackoverflow. <br>\nUsing a DL model (BEART) I will try to predict if a question posted on the site is at risk of being closed. <br>\nQuestions that don't meet the stockoverfull standard are closed because they are irrelevant, poorly written, etc (about 6% of the questions posted). <br>\n\nMain parts:\n* Import libraries and load data from database\n* Data exploration\n* Data cleaning and preparation\n* Building our custom BERT model\n* Training\n* Evaluate the model results\n* Conclusions","metadata":{}},{"cell_type":"markdown","source":" # 2. Import libraries and load data from database","metadata":{"execution":{"iopub.status.busy":"2022-08-26T17:50:45.134620Z","iopub.execute_input":"2022-08-26T17:50:45.135077Z","iopub.status.idle":"2022-08-26T17:50:45.143821Z","shell.execute_reply.started":"2022-08-26T17:50:45.135044Z","shell.execute_reply":"2022-08-26T17:50:45.141757Z"}}},{"cell_type":"markdown","source":"### libraries","metadata":{}},{"cell_type":"code","source":"# libraries\n\n# data manipulation\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport re\nimport os\nfrom pathlib import Path  \nimport datetime\nimport seaborn as sn\n\n# import nlp Tokenizer/tools\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import words\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('omw-1.4')\nfrom transformers import BertTokenizer\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom transformers import BertModel,DistilBertModel\nfrom transformers import AdamW\n\n# Evaluation and data prep\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n# setup\npd.set_option('display.max_colwidth', None) # want to see all info in a cell","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T19:23:29.085782Z","iopub.execute_input":"2022-08-31T19:23:29.086529Z","iopub.status.idle":"2022-08-31T19:23:37.941398Z","shell.execute_reply.started":"2022-08-31T19:23:29.086412Z","shell.execute_reply":"2022-08-31T19:23:37.940348Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Loading data","metadata":{}},{"cell_type":"code","source":"# Loading and printing head\ndf_stock_over_raw = pd.read_csv(\"../input/predict-closed-questions-on-stack-overflow/train-sample.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:37.943576Z","iopub.execute_input":"2022-08-31T19:23:37.944263Z","iopub.status.idle":"2022-08-31T19:23:42.010539Z","shell.execute_reply.started":"2022-08-31T19:23:37.944224Z","shell.execute_reply":"2022-08-31T19:23:42.006100Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_stock_over_raw[0:3]","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:42.011639Z","iopub.execute_input":"2022-08-31T19:23:42.012018Z","iopub.status.idle":"2022-08-31T19:23:42.080076Z","shell.execute_reply.started":"2022-08-31T19:23:42.011984Z","shell.execute_reply":"2022-08-31T19:23:42.075179Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":" # 3. Data exploration","metadata":{"execution":{"iopub.status.busy":"2022-08-26T17:57:56.413030Z","iopub.execute_input":"2022-08-26T17:57:56.414561Z","iopub.status.idle":"2022-08-26T17:57:56.421258Z","shell.execute_reply.started":"2022-08-26T17:57:56.414492Z","shell.execute_reply":"2022-08-26T17:57:56.419759Z"}}},{"cell_type":"markdown","source":"### Lets take a look at the diffrent columns","metadata":{"execution":{"iopub.status.busy":"2022-08-30T07:37:14.305483Z","iopub.execute_input":"2022-08-30T07:37:14.306161Z","iopub.status.idle":"2022-08-30T07:37:14.311425Z","shell.execute_reply.started":"2022-08-30T07:37:14.306117Z","shell.execute_reply":"2022-08-30T07:37:14.310136Z"}}},{"cell_type":"code","source":"df_stock_over_raw.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:42.082337Z","iopub.execute_input":"2022-08-31T19:23:42.082646Z","iopub.status.idle":"2022-08-31T19:23:42.331960Z","shell.execute_reply.started":"2022-08-31T19:23:42.082616Z","shell.execute_reply":"2022-08-31T19:23:42.322717Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We have 140,272 data points, lets see how many are closed vs open.","metadata":{}},{"cell_type":"markdown","source":"### Lets take a look at the ratio of diffrent status values","metadata":{"execution":{"iopub.status.busy":"2022-08-30T07:37:14.305483Z","iopub.execute_input":"2022-08-30T07:37:14.306161Z","iopub.status.idle":"2022-08-30T07:37:14.311425Z","shell.execute_reply.started":"2022-08-30T07:37:14.306117Z","shell.execute_reply":"2022-08-30T07:37:14.310136Z"}}},{"cell_type":"code","source":"df_stock_over_raw.groupby(['OpenStatus']).count()['PostId']/140272","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:42.333342Z","iopub.execute_input":"2022-08-31T19:23:42.333734Z","iopub.status.idle":"2022-08-31T19:23:42.552304Z","shell.execute_reply.started":"2022-08-31T19:23:42.333679Z","shell.execute_reply":"2022-08-31T19:23:42.548654Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Lets see the status events by %\nstock_over_status_split = df_stock_over_raw.groupby(['OpenStatus']).count()['PostId']/140272\nstock_over_status_split.sort_values(ascending=False).plot(kind=\"bar\")\nprint('Question status %')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:42.553917Z","iopub.execute_input":"2022-08-31T19:23:42.559033Z","iopub.status.idle":"2022-08-31T19:23:43.483787Z","shell.execute_reply.started":"2022-08-31T19:23:42.558984Z","shell.execute_reply":"2022-08-31T19:23:43.482681Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We see that about half is open in our sample.","metadata":{"execution":{"iopub.status.busy":"2022-08-26T18:25:25.059360Z","iopub.execute_input":"2022-08-26T18:25:25.059785Z","iopub.status.idle":"2022-08-26T18:25:25.064885Z","shell.execute_reply.started":"2022-08-26T18:25:25.059753Z","shell.execute_reply":"2022-08-26T18:25:25.063892Z"}}},{"cell_type":"code","source":"# Lets see if we have more features with a small amount of values\ndf_stock_over_raw.nunique().sort_values()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:43.485283Z","iopub.execute_input":"2022-08-31T19:23:43.486068Z","iopub.status.idle":"2022-08-31T19:23:43.956540Z","shell.execute_reply.started":"2022-08-31T19:23:43.486025Z","shell.execute_reply":"2022-08-31T19:23:43.955618Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# lets see the first 3 headers/body\nfor i in range(1):\n    print('Question number {}: \\n\\nheader is: \\n{}\\nbody: \\n{} \\n'.format(i+1,df_stock_over_raw['Title'].iloc[i],df_stock_over_raw['BodyMarkdown'].iloc[i]))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:43.958501Z","iopub.execute_input":"2022-08-31T19:23:43.959319Z","iopub.status.idle":"2022-08-31T19:23:43.967553Z","shell.execute_reply.started":"2022-08-31T19:23:43.959280Z","shell.execute_reply":"2022-08-31T19:23:43.966364Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Lets take a look at the amount of words we have in the first 95% data points","metadata":{"execution":{"iopub.status.busy":"2022-08-30T07:37:14.305483Z","iopub.execute_input":"2022-08-30T07:37:14.306161Z","iopub.status.idle":"2022-08-30T07:37:14.311425Z","shell.execute_reply.started":"2022-08-30T07:37:14.306117Z","shell.execute_reply":"2022-08-30T07:37:14.310136Z"}}},{"cell_type":"code","source":"full_text = df_stock_over_raw['Title'] + df_stock_over_raw['BodyMarkdown']\nfull_text_count = full_text.str.split().str.len()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:43.969076Z","iopub.execute_input":"2022-08-31T19:23:43.969720Z","iopub.status.idle":"2022-08-31T19:23:46.321425Z","shell.execute_reply.started":"2022-08-31T19:23:43.969656Z","shell.execute_reply":"2022-08-31T19:23:46.320387Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"full_text_count[full_text_count<full_text_count.quantile(0.95)].hist(bins=10)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:46.325459Z","iopub.execute_input":"2022-08-31T19:23:46.325772Z","iopub.status.idle":"2022-08-31T19:23:46.928627Z","shell.execute_reply.started":"2022-08-31T19:23:46.325745Z","shell.execute_reply":"2022-08-31T19:23:46.927722Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"So we can easily see that most of our data is below BERT's 512 tolkens limit, thats good!","metadata":{}},{"cell_type":"markdown","source":"# 4. Data cleaning and preparation","metadata":{}},{"cell_type":"code","source":"# Lets start with creating a new column for combining head and body\ndf_stock_over_raw['text'] = df_stock_over_raw['Title'] + ' ' +df_stock_over_raw['BodyMarkdown']","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:46.929908Z","iopub.execute_input":"2022-08-31T19:23:46.930475Z","iopub.status.idle":"2022-08-31T19:23:47.017629Z","shell.execute_reply.started":"2022-08-31T19:23:46.930436Z","shell.execute_reply":"2022-08-31T19:23:47.016214Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# looks good!\ndf_stock_over_raw[0:1][['text','BodyMarkdown','Title']]","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:47.019050Z","iopub.execute_input":"2022-08-31T19:23:47.019505Z","iopub.status.idle":"2022-08-31T19:23:47.031388Z","shell.execute_reply.started":"2022-08-31T19:23:47.019468Z","shell.execute_reply":"2022-08-31T19:23:47.030256Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Lets create a new df with only the relavent columns\ndf_for_bert = df_stock_over_raw[['text','OpenStatus']]","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:47.033206Z","iopub.execute_input":"2022-08-31T19:23:47.033915Z","iopub.status.idle":"2022-08-31T19:23:47.113895Z","shell.execute_reply.started":"2022-08-31T19:23:47.033871Z","shell.execute_reply":"2022-08-31T19:23:47.112911Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Lets change the names of our text and lables\ndf_for_bert.columns = ['text','label']\ndf_for_bert.columns","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:47.117422Z","iopub.execute_input":"2022-08-31T19:23:47.117756Z","iopub.status.idle":"2022-08-31T19:23:47.128078Z","shell.execute_reply.started":"2022-08-31T19:23:47.117727Z","shell.execute_reply":"2022-08-31T19:23:47.126768Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# mapping the category name to numbers\n\nlabels_status = {'open': 0, \n           'too localized': 1,\n           'not a real question': 2,\n           'off topic': 3,\n           'not constructive':4}\n\ndf_for_bert['label'] = df_for_bert['label'].map(labels_status)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:47.130433Z","iopub.execute_input":"2022-08-31T19:23:47.131232Z","iopub.status.idle":"2022-08-31T19:23:47.153127Z","shell.execute_reply.started":"2022-08-31T19:23:47.131195Z","shell.execute_reply":"2022-08-31T19:23:47.152184Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Split train dataset into train, validation and test sets","metadata":{}},{"cell_type":"code","source":"train_text, temp_text, train_labels, temp_labels = train_test_split(df_for_bert['text'], df_for_bert['label'], \n                                                                    random_state=42, \n                                                                    test_size=0.3, \n                                                                    stratify=df_for_bert['label'])\n\n# we will use temp_text and temp_labels to create validation and test set\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=42, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:47.161374Z","iopub.execute_input":"2022-08-31T19:23:47.162113Z","iopub.status.idle":"2022-08-31T19:23:47.195338Z","shell.execute_reply.started":"2022-08-31T19:23:47.162057Z","shell.execute_reply":"2022-08-31T19:23:47.194238Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Import BERT Model and BERT Tokenizer","metadata":{}},{"cell_type":"code","source":"# import BERT model\nbert = BertModel.from_pretrained('bert-base-uncased')\n\n# Loading the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:23:47.196881Z","iopub.execute_input":"2022-08-31T19:23:47.197223Z","iopub.status.idle":"2022-08-31T19:24:13.311763Z","shell.execute_reply.started":"2022-08-31T19:23:47.197190Z","shell.execute_reply":"2022-08-31T19:24:13.310675Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Preparing the tokenizers","metadata":{}},{"cell_type":"code","source":"# setting the maximum tokens possible for BERT 512 or less if GPU space isnt sufficient\nmax_seq_len = 512\n\n# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:32:04.616746Z","iopub.execute_input":"2022-08-31T20:32:04.617411Z","iopub.status.idle":"2022-08-31T20:35:49.749058Z","shell.execute_reply.started":"2022-08-31T20:32:04.617374Z","shell.execute_reply":"2022-08-31T20:35:49.748055Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Casting type to torch tensor","metadata":{}},{"cell_type":"code","source":"# Train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_y = torch.tensor(train_labels.tolist())\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\n\n# Validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_y = torch.tensor(val_labels.tolist())\nval_mask = torch.tensor(tokens_val['attention_mask'])\n\n# Test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_y = torch.tensor(test_labels.tolist())\ntest_mask = torch.tensor(tokens_test['attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:35:49.752561Z","iopub.execute_input":"2022-08-31T20:35:49.752972Z","iopub.status.idle":"2022-08-31T20:35:52.726959Z","shell.execute_reply.started":"2022-08-31T20:35:49.752944Z","shell.execute_reply":"2022-08-31T20:35:52.725914Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### Creating DataLoaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:35:52.728498Z","iopub.execute_input":"2022-08-31T20:35:52.728947Z","iopub.status.idle":"2022-08-31T20:35:52.746853Z","shell.execute_reply.started":"2022-08-31T20:35:52.728905Z","shell.execute_reply":"2022-08-31T20:35:52.745650Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# 5. Building our custom BERT model","metadata":{}},{"cell_type":"markdown","source":"### Class creation","metadata":{}},{"cell_type":"code","source":"'''\nClass BERT_Arch is based on pre-trained model BERT, its designed to output five classes\n\n'''\n\nclass BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n        \n      \n        super(BERT_Arch, self).__init__()\n\n        self.bert = bert \n      \n        # dropout layer\n        self.dropout = nn.Dropout(0.1)\n      \n       # relu activation function\n        self.relu =  nn.ReLU()\n\n        # dense layer 1\n        self.fc1 = nn.Linear(768,512)\n      \n        # dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512,5)\n\n        #softmax activation function\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n        \n\n        #pass the inputs to the model  \n        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n        \n        x = self.fc1(cls_hs)\n\n        x = self.relu(x)\n\n        x = self.dropout(x)\n\n        # output layer\n        x = self.fc2(x)\n      \n        # apply softmax activation\n        x = self.softmax(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:35:52.749642Z","iopub.execute_input":"2022-08-31T20:35:52.751355Z","iopub.status.idle":"2022-08-31T20:35:52.761672Z","shell.execute_reply.started":"2022-08-31T20:35:52.751318Z","shell.execute_reply":"2022-08-31T20:35:52.760527Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Instantiation the model and transfering to GPU mode","metadata":{"execution":{"iopub.status.busy":"2022-08-30T22:43:16.419639Z","iopub.execute_input":"2022-08-30T22:43:16.420123Z","iopub.status.idle":"2022-08-30T22:43:16.428941Z","shell.execute_reply.started":"2022-08-30T22:43:16.420078Z","shell.execute_reply":"2022-08-30T22:43:16.427769Z"}}},{"cell_type":"code","source":"# specify GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)\n\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = 1e-3)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:35:52.763201Z","iopub.execute_input":"2022-08-31T20:35:52.763567Z","iopub.status.idle":"2022-08-31T20:35:52.787225Z","shell.execute_reply.started":"2022-08-31T20:35:52.763522Z","shell.execute_reply":"2022-08-31T20:35:52.785624Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"### Creating the train Class","metadata":{}},{"cell_type":"code","source":"'''\nFunction for train the model\n\nInput:  None\n\nOutput: \n        avg_loss (Float) - the average loss\n        total_preds (List) - List of model predictions\n'''\n\n# function to train the model\ndef train():\n  \n    model.train()\n\n    total_loss, total_accuracy = 0, 0\n  \n    # empty list to save model predictions\n    total_preds=[]\n  \n    # iterate over batches\n    for step,batch in enumerate(train_dataloader):\n    \n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n \n        sent_id, mask, labels = batch\n\n        # clear previously calculated gradients \n        model.zero_grad()        \n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n  \n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n\n        # append the model predictions\n        total_preds.append(preds)\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(train_dataloader)\n  \n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:28:11.388295Z","iopub.execute_input":"2022-08-31T19:28:11.388683Z","iopub.status.idle":"2022-08-31T19:28:11.399881Z","shell.execute_reply.started":"2022-08-31T19:28:11.388646Z","shell.execute_reply":"2022-08-31T19:28:11.398549Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Creating the evaluate calss","metadata":{}},{"cell_type":"code","source":"'''\nFunction for evaluating the model\n\nInput:  None\n\nOutput: \n        avg_loss (Float) - the average loss\n        total_preds (List) - List of model predictions\n'''\n\ndef evaluate():\n    \n    print(\"\\nEvaluating...\")\n    \n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n  \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step,batch in enumerate(val_dataloader):\n    \n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n    \n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n        \n        # model predictions\n        preds = model(sent_id, mask)\n\n        # compute the validation loss between actual and predicted values\n        loss = cross_entropy(preds,labels)\n\n        total_loss = total_loss + loss.item()\n\n        preds = preds.detach().cpu().numpy()\n\n        total_preds.append(preds)\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:28:11.401195Z","iopub.execute_input":"2022-08-31T19:28:11.402315Z","iopub.status.idle":"2022-08-31T19:28:11.419472Z","shell.execute_reply.started":"2022-08-31T19:28:11.402279Z","shell.execute_reply":"2022-08-31T19:28:11.418270Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# 6. Training","metadata":{}},{"cell_type":"markdown","source":"### Freeze BERT Parameters","metadata":{}},{"cell_type":"code","source":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-08-31T18:43:49.117678Z","iopub.execute_input":"2022-08-31T18:43:49.118497Z","iopub.status.idle":"2022-08-31T18:43:49.131324Z","shell.execute_reply.started":"2022-08-31T18:43:49.118461Z","shell.execute_reply":"2022-08-31T18:43:49.130371Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Setting up the loss function and number of epochs","metadata":{}},{"cell_type":"code","source":"# loss function\ncross_entropy  = nn.NLLLoss() \n\n# number of training epochs\nepochs = 3","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:28:11.420940Z","iopub.execute_input":"2022-08-31T19:28:11.421305Z","iopub.status.idle":"2022-08-31T19:28:11.433451Z","shell.execute_reply.started":"2022-08-31T19:28:11.421270Z","shell.execute_reply":"2022-08-31T19:28:11.432433Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Training and tarning/validation losses","metadata":{}},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\nmodel = model.to(device)\n\n# lists for validation and traning loesses\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T19:28:11.435275Z","iopub.execute_input":"2022-08-31T19:28:11.435660Z","iopub.status.idle":"2022-08-31T20:18:30.876213Z","shell.execute_reply.started":"2022-08-31T19:28:11.435625Z","shell.execute_reply":"2022-08-31T20:18:30.874768Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# 7. Evaluate the model results","metadata":{}},{"cell_type":"code","source":"# Push the model to GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\npred_list = []\ncounty = 0\n\n\n# Get predictions for test data, create a list of predictions to evaluate later\nwith torch.no_grad():\n    model.eval()\n    for s,m in zip(test_seq,test_mask):   \n        county += 1\n        preds = model(s.to(device).reshape(1,512), m.to(device).reshape(1,512))\n        preds = preds.detach().cpu().numpy()\n        \n        if county==10000:\n            print('10k')\n        if county==20000:\n            print('20k')\n        \n        pred_list.append(preds[0])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:36:46.373735Z","iopub.execute_input":"2022-08-31T20:36:46.374293Z","iopub.status.idle":"2022-08-31T20:39:26.126742Z","shell.execute_reply.started":"2022-08-31T20:36:46.374249Z","shell.execute_reply":"2022-08-31T20:39:26.125676Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Pick the higest score for prediction\npred_list = np.argmax(pred_list, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:39:26.133489Z","iopub.execute_input":"2022-08-31T20:39:26.134174Z","iopub.status.idle":"2022-08-31T20:39:26.157926Z","shell.execute_reply.started":"2022-08-31T20:39:26.134133Z","shell.execute_reply":"2022-08-31T20:39:26.157051Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# model's performance\nprint(classification_report(test_y, pred_list))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:40:04.812518Z","iopub.execute_input":"2022-08-31T20:40:04.812904Z","iopub.status.idle":"2022-08-31T20:40:04.837341Z","shell.execute_reply.started":"2022-08-31T20:40:04.812872Z","shell.execute_reply":"2022-08-31T20:40:04.836374Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# confusion matrix\n\ndf_cm = pd.crosstab(pred_list,test_y,colnames=['Real value'], rownames=['Predicted value'])\nsn.heatmap(df_cm, annot=True, fmt='.5g')","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:40:15.700426Z","iopub.execute_input":"2022-08-31T20:40:15.701206Z","iopub.status.idle":"2022-08-31T20:40:15.999600Z","shell.execute_reply.started":"2022-08-31T20:40:15.701168Z","shell.execute_reply":"2022-08-31T20:40:15.998615Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Saving and loading the models","metadata":{}},{"cell_type":"code","source":"#save BERT\n#torch.save(model.state_dict(),'model_state_v3.pth')","metadata":{"execution":{"iopub.status.busy":"2022-08-30T22:34:41.619197Z","iopub.execute_input":"2022-08-30T22:34:41.619874Z","iopub.status.idle":"2022-08-30T22:34:42.454955Z","shell.execute_reply.started":"2022-08-30T22:34:41.619809Z","shell.execute_reply":"2022-08-30T22:34:42.453916Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"code","source":"# load BERT\nmodel = BERT_Arch(bert)\nmodel.load_state_dict(torch.load('../input/model-state-v3/model_state_v1.pth'))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T20:35:52.788907Z","iopub.execute_input":"2022-08-31T20:35:52.789267Z","iopub.status.idle":"2022-08-31T20:35:59.059310Z","shell.execute_reply.started":"2022-08-31T20:35:52.789229Z","shell.execute_reply":"2022-08-31T20:35:59.058396Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# 8. Conclusions","metadata":{"execution":{"iopub.status.busy":"2022-08-31T21:12:13.647033Z","iopub.execute_input":"2022-08-31T21:12:13.647406Z","iopub.status.idle":"2022-08-31T21:12:13.672013Z","shell.execute_reply.started":"2022-08-31T21:12:13.647330Z","shell.execute_reply":"2022-08-31T21:12:13.670894Z"}}},{"cell_type":"markdown","source":"I ran the training loop three times in order to get a better result. <br>\nWe can see the model definitely \"learned\" and gets a better score than random prediction.\n\nI see two ways to use the model:\n\n1. Approve any post that comes in and then take only the question that predicted anything that is not 0 (open question), then a human will take a look and decide if the question should be closed. In this case, we can get 12%-33% of type 2/3/4 questions denied and 0% of type 1. The rest will be falsy approved.\n2. Approve type 0 prediction automatically, it will mean that we will lose 7% of good questions and give false approval to about 50% of type 2,3,4 of questions and 100% of type 1. This is the easiest way to implment the model, but will probely have to approve the results before.\n\nI think I will keep training the model and maybe change the drop rate/model feature in order to improve my results, especially in category 1.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}